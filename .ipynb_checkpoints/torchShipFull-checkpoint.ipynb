{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import mnist\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchinfo import summary\n",
    "from torch.nn.parameter import Parameter\n",
    "import random\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Net                                      --\n",
       "├─Linear: 1-1                            2,560\n",
       "├─ReLU: 1-2                              --\n",
       "├─Linear: 1-3                            131,328\n",
       "├─ReLU: 1-4                              --\n",
       "├─Linear: 1-5                            32,896\n",
       "├─ReLU: 1-6                              --\n",
       "├─Linear: 1-7                            258\n",
       "├─Linear: 1-8                            3\n",
       "=================================================================\n",
       "Total params: 167,045\n",
       "Trainable params: 167,045\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input, fc1, fc2, fc3, y, out):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc1 = nn.Linear(input, fc1)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \"\"\"创建mask枝剪\n",
    "        #custom_mask = torch.randint(0, 2, size=self.fc1.weight.shape)\n",
    "        \n",
    "        custom_mask = torch.tensor([[1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, 1, 1],\n",
    "        [1, 0, 1, 0],\n",
    "        [1, 1, 1, 1],\n",
    "        [0, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [0, 0, 1, 0]])\n",
    "        print (custom_mask)\n",
    "        \n",
    "        prune.custom_from_mask(self.fc1, 'weight', mask=custom_mask)\n",
    "        \"\"\"\n",
    "\n",
    "        self.fc2 = nn.Linear(fc1, fc2)\n",
    "        self.act2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.fc3 = nn.Linear(fc2, fc3)\n",
    "        self.act3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.y = nn.Linear(fc3, y)\n",
    "        \n",
    "        self.out = nn.Linear(y, out, bias = True)\n",
    "        \n",
    "        \n",
    "        \"\"\"赋权重\n",
    "        w_init = torch.ones(self.fc4.weight.shape)\n",
    "        for i in range(len(w_init)):\n",
    "            for j in range(len(w_init[i])):\n",
    "                w_init[i][j] = float(random.randint(0, 1))\n",
    "        #w_init = torch.rand(self.fc4.weight.shape)\n",
    "        self.fc4.weight = Parameter(w_init)\n",
    "        \"\"\"\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        x = self.act3(self.fc3(x))\n",
    "        x = self.y(x)\n",
    "        x = self.out(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x \n",
    "\n",
    "# 构建网络\n",
    "net = Net(4, 512, 256, 128, 2, 1)\n",
    "\n",
    "\"\"\"\n",
    "#冻结层\n",
    "for name, param in net.named_parameters():\n",
    "    if \"y\" in name:\n",
    "        param.requires_grad = False\n",
    "\"\"\"        \n",
    "summary(net)\n",
    "#print (net.y.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('./data/ship_fuel_consumption_NN_1.csv')\n",
    "d = d.dropna(axis=0,how='any')\n",
    "d.to_csv('./data/ship_fuel_consumption_NN_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipDataset(Dataset):\n",
    "    \"\"\" 数据集演示 \"\"\"\n",
    "    def __init__(self, csv_file, x1, x2):\n",
    "        \"\"\"实现初始化方法，在初始化的时候将数据读载入\"\"\"\n",
    "        self.df = pd.read_csv(csv_file).iloc[x1:x2]\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回df的长度\n",
    "        '''\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        #data = ((self.df.iloc[idx].STW, self.df.iloc[idx].avgLEVEL, self.df.iloc[idx].headwind, self.df.iloc[idx].crosswind), self.df.iloc[idx].fuelConsumption)\n",
    "        input0 = torch.tensor(np.expand_dims(self.df.iloc[idx].STW, 0))\n",
    "        input1 = torch.tensor(np.expand_dims(self.df.iloc[idx].avgLEVEL, 0))\n",
    "        input2 = torch.tensor(np.expand_dims(self.df.iloc[idx].headwind, 0))\n",
    "        input3 = torch.tensor(np.expand_dims(self.df.iloc[idx].crosswind, 0))\n",
    "        data0 = torch.cat((input0, input1, input2, input3))\n",
    "        data1 = self.df.iloc[idx].fuelConsumption\n",
    "        tup = (data0, data1)\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ShipDataset('./data/ship_fuel_consumption_NN_2.csv', 0, 140000)\n",
    "test = ShipDataset('./data/ship_fuel_consumption_NN_2.csv', 140001, 166587)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数--mse就是均方差\n",
    "criterion = nn.MSELoss()\n",
    "# 定义优化器---随机梯度下降\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch-1-Batch-200: Train: Loss-0.0110, Accuracy-0.0469\n",
      "[INFO] Epoch-1-Batch-400: Train: Loss-0.0075, Accuracy-0.0469\n",
      "[INFO] Epoch-1-Batch-600: Train: Loss-0.0163, Accuracy-0.0312\n",
      "[INFO] Epoch-1-Batch-800: Train: Loss-0.0074, Accuracy-0.0469\n",
      "[INFO] Epoch-1-Batch-1000: Train: Loss-0.0048, Accuracy-0.0625\n",
      "[INFO] Epoch-1-Batch-1200: Train: Loss-0.0040, Accuracy-0.0938\n",
      "[INFO] Epoch-1-Batch-1400: Train: Loss-0.0054, Accuracy-0.0938\n",
      "[INFO] Epoch-1-Batch-1600: Train: Loss-0.0092, Accuracy-0.1250\n",
      "[INFO] Epoch-1-Batch-1800: Train: Loss-0.0034, Accuracy-0.0000\n",
      "[INFO] Epoch-1-Batch-2000: Train: Loss-0.0046, Accuracy-0.0938\n",
      "[INFO] Epoch-1: Train: Loss-0.0296, Accuracy-0.0568 | Test: Loss-0.0062, Accuracy-0.0888\n",
      "[INFO] Epoch-2-Batch-200: Train: Loss-0.0053, Accuracy-0.0625\n",
      "[INFO] Epoch-2-Batch-400: Train: Loss-0.0061, Accuracy-0.0469\n",
      "[INFO] Epoch-2-Batch-600: Train: Loss-0.0064, Accuracy-0.0469\n",
      "[INFO] Epoch-2-Batch-800: Train: Loss-0.0047, Accuracy-0.0312\n",
      "[INFO] Epoch-2-Batch-1000: Train: Loss-0.0067, Accuracy-0.0781\n",
      "[INFO] Epoch-2-Batch-1200: Train: Loss-0.0038, Accuracy-0.1094\n",
      "[INFO] Epoch-2-Batch-1400: Train: Loss-0.0051, Accuracy-0.0625\n",
      "[INFO] Epoch-2-Batch-1600: Train: Loss-0.0064, Accuracy-0.0625\n",
      "[INFO] Epoch-2-Batch-1800: Train: Loss-0.0044, Accuracy-0.0312\n",
      "[INFO] Epoch-2-Batch-2000: Train: Loss-0.0063, Accuracy-0.0781\n",
      "[INFO] Epoch-2: Train: Loss-0.0059, Accuracy-0.0689 | Test: Loss-0.0107, Accuracy-0.0213\n",
      "[INFO] Epoch-3-Batch-200: Train: Loss-0.0100, Accuracy-0.0000\n",
      "[INFO] Epoch-3-Batch-400: Train: Loss-0.0066, Accuracy-0.0938\n",
      "[INFO] Epoch-3-Batch-600: Train: Loss-0.0068, Accuracy-0.0625\n",
      "[INFO] Epoch-3-Batch-800: Train: Loss-0.0050, Accuracy-0.1250\n",
      "[INFO] Epoch-3-Batch-1000: Train: Loss-0.0064, Accuracy-0.0781\n",
      "[INFO] Epoch-3-Batch-1200: Train: Loss-0.0074, Accuracy-0.0625\n",
      "[INFO] Epoch-3-Batch-1400: Train: Loss-0.0069, Accuracy-0.0469\n",
      "[INFO] Epoch-3-Batch-1600: Train: Loss-0.0056, Accuracy-0.0625\n",
      "[INFO] Epoch-3-Batch-1800: Train: Loss-0.0046, Accuracy-0.1094\n",
      "[INFO] Epoch-3-Batch-2000: Train: Loss-0.0064, Accuracy-0.0781\n",
      "[INFO] Epoch-3: Train: Loss-0.0055, Accuracy-0.0704 | Test: Loss-0.0064, Accuracy-0.0742\n",
      "[INFO] Epoch-4-Batch-200: Train: Loss-0.0023, Accuracy-0.1250\n",
      "[INFO] Epoch-4-Batch-400: Train: Loss-0.0036, Accuracy-0.0625\n",
      "[INFO] Epoch-4-Batch-600: Train: Loss-0.0083, Accuracy-0.1094\n",
      "[INFO] Epoch-4-Batch-800: Train: Loss-0.0059, Accuracy-0.0938\n",
      "[INFO] Epoch-4-Batch-1000: Train: Loss-0.0042, Accuracy-0.0938\n",
      "[INFO] Epoch-4-Batch-1200: Train: Loss-0.0057, Accuracy-0.0312\n",
      "[INFO] Epoch-4-Batch-1400: Train: Loss-0.0044, Accuracy-0.0781\n",
      "[INFO] Epoch-4-Batch-1600: Train: Loss-0.0033, Accuracy-0.0625\n",
      "[INFO] Epoch-4-Batch-1800: Train: Loss-0.0084, Accuracy-0.0781\n",
      "[INFO] Epoch-4-Batch-2000: Train: Loss-0.0050, Accuracy-0.1094\n",
      "[INFO] Epoch-4: Train: Loss-0.0051, Accuracy-0.0763 | Test: Loss-0.0052, Accuracy-0.1652\n",
      "[INFO] Epoch-5-Batch-200: Train: Loss-0.0042, Accuracy-0.1094\n",
      "[INFO] Epoch-5-Batch-400: Train: Loss-0.0017, Accuracy-0.1406\n",
      "[INFO] Epoch-5-Batch-600: Train: Loss-0.0044, Accuracy-0.0781\n",
      "[INFO] Epoch-5-Batch-800: Train: Loss-0.0048, Accuracy-0.0625\n",
      "[INFO] Epoch-5-Batch-1000: Train: Loss-0.0049, Accuracy-0.0781\n",
      "[INFO] Epoch-5-Batch-1200: Train: Loss-0.0054, Accuracy-0.0625\n",
      "[INFO] Epoch-5-Batch-1400: Train: Loss-0.0064, Accuracy-0.1094\n",
      "[INFO] Epoch-5-Batch-1600: Train: Loss-0.0038, Accuracy-0.1250\n",
      "[INFO] Epoch-5-Batch-1800: Train: Loss-0.0039, Accuracy-0.0469\n",
      "[INFO] Epoch-5-Batch-2000: Train: Loss-0.0045, Accuracy-0.0938\n",
      "[INFO] Epoch-5: Train: Loss-0.0049, Accuracy-0.0813 | Test: Loss-0.0049, Accuracy-0.1124\n",
      "[INFO] Epoch-6-Batch-200: Train: Loss-0.0051, Accuracy-0.0469\n",
      "[INFO] Epoch-6-Batch-400: Train: Loss-0.0055, Accuracy-0.0625\n",
      "[INFO] Epoch-6-Batch-600: Train: Loss-0.0069, Accuracy-0.0469\n",
      "[INFO] Epoch-6-Batch-800: Train: Loss-0.0055, Accuracy-0.0625\n",
      "[INFO] Epoch-6-Batch-1000: Train: Loss-0.0059, Accuracy-0.0781\n",
      "[INFO] Epoch-6-Batch-1200: Train: Loss-0.0081, Accuracy-0.0156\n",
      "[INFO] Epoch-6-Batch-1400: Train: Loss-0.0038, Accuracy-0.1719\n",
      "[INFO] Epoch-6-Batch-1600: Train: Loss-0.0027, Accuracy-0.0938\n",
      "[INFO] Epoch-6-Batch-1800: Train: Loss-0.0041, Accuracy-0.2188\n",
      "[INFO] Epoch-6-Batch-2000: Train: Loss-0.0064, Accuracy-0.0781\n",
      "[INFO] Epoch-6: Train: Loss-0.0046, Accuracy-0.0841 | Test: Loss-0.0063, Accuracy-0.0311\n",
      "[INFO] Epoch-7-Batch-200: Train: Loss-0.0044, Accuracy-0.0938\n",
      "[INFO] Epoch-7-Batch-400: Train: Loss-0.0030, Accuracy-0.1719\n",
      "[INFO] Epoch-7-Batch-600: Train: Loss-0.0029, Accuracy-0.0312\n",
      "[INFO] Epoch-7-Batch-800: Train: Loss-0.0025, Accuracy-0.0625\n",
      "[INFO] Epoch-7-Batch-1000: Train: Loss-0.0050, Accuracy-0.0938\n",
      "[INFO] Epoch-7-Batch-1200: Train: Loss-0.0040, Accuracy-0.1250\n",
      "[INFO] Epoch-7-Batch-1400: Train: Loss-0.0032, Accuracy-0.0625\n",
      "[INFO] Epoch-7-Batch-1600: Train: Loss-0.0055, Accuracy-0.1094\n",
      "[INFO] Epoch-7-Batch-1800: Train: Loss-0.0038, Accuracy-0.0469\n",
      "[INFO] Epoch-7-Batch-2000: Train: Loss-0.0029, Accuracy-0.0469\n",
      "[INFO] Epoch-7: Train: Loss-0.0045, Accuracy-0.0871 | Test: Loss-0.0054, Accuracy-0.0605\n",
      "[INFO] Epoch-8-Batch-200: Train: Loss-0.0038, Accuracy-0.0781\n",
      "[INFO] Epoch-8-Batch-400: Train: Loss-0.0045, Accuracy-0.1562\n",
      "[INFO] Epoch-8-Batch-600: Train: Loss-0.0075, Accuracy-0.0469\n",
      "[INFO] Epoch-8-Batch-800: Train: Loss-0.0027, Accuracy-0.1094\n",
      "[INFO] Epoch-8-Batch-1000: Train: Loss-0.0023, Accuracy-0.1250\n",
      "[INFO] Epoch-8-Batch-1200: Train: Loss-0.0030, Accuracy-0.0781\n",
      "[INFO] Epoch-8-Batch-1400: Train: Loss-0.0054, Accuracy-0.1094\n",
      "[INFO] Epoch-8-Batch-1600: Train: Loss-0.0067, Accuracy-0.0781\n",
      "[INFO] Epoch-8-Batch-1800: Train: Loss-0.0042, Accuracy-0.1250\n",
      "[INFO] Epoch-8-Batch-2000: Train: Loss-0.0020, Accuracy-0.1406\n",
      "[INFO] Epoch-8: Train: Loss-0.0044, Accuracy-0.0903 | Test: Loss-0.0048, Accuracy-0.1029\n",
      "[INFO] Epoch-9-Batch-200: Train: Loss-0.0036, Accuracy-0.0938\n",
      "[INFO] Epoch-9-Batch-400: Train: Loss-0.0055, Accuracy-0.1719\n",
      "[INFO] Epoch-9-Batch-600: Train: Loss-0.0037, Accuracy-0.1094\n",
      "[INFO] Epoch-9-Batch-800: Train: Loss-0.0036, Accuracy-0.0938\n",
      "[INFO] Epoch-9-Batch-1000: Train: Loss-0.0033, Accuracy-0.0625\n",
      "[INFO] Epoch-9-Batch-1200: Train: Loss-0.0056, Accuracy-0.1094\n",
      "[INFO] Epoch-9-Batch-1400: Train: Loss-0.0043, Accuracy-0.0938\n",
      "[INFO] Epoch-9-Batch-1600: Train: Loss-0.0084, Accuracy-0.0469\n",
      "[INFO] Epoch-9-Batch-1800: Train: Loss-0.0032, Accuracy-0.1719\n",
      "[INFO] Epoch-9-Batch-2000: Train: Loss-0.0024, Accuracy-0.0938\n",
      "[INFO] Epoch-9: Train: Loss-0.0043, Accuracy-0.0925 | Test: Loss-0.0047, Accuracy-0.1018\n",
      "[INFO] Epoch-10-Batch-200: Train: Loss-0.0026, Accuracy-0.0781\n",
      "[INFO] Epoch-10-Batch-400: Train: Loss-0.0021, Accuracy-0.0938\n",
      "[INFO] Epoch-10-Batch-600: Train: Loss-0.0066, Accuracy-0.0625\n",
      "[INFO] Epoch-10-Batch-800: Train: Loss-0.0062, Accuracy-0.0469\n",
      "[INFO] Epoch-10-Batch-1000: Train: Loss-0.0026, Accuracy-0.1250\n",
      "[INFO] Epoch-10-Batch-1200: Train: Loss-0.0073, Accuracy-0.0625\n",
      "[INFO] Epoch-10-Batch-1400: Train: Loss-0.0072, Accuracy-0.0625\n",
      "[INFO] Epoch-10-Batch-1600: Train: Loss-0.0033, Accuracy-0.0938\n",
      "[INFO] Epoch-10-Batch-1800: Train: Loss-0.0043, Accuracy-0.1094\n",
      "[INFO] Epoch-10-Batch-2000: Train: Loss-0.0026, Accuracy-0.1250\n",
      "[INFO] Epoch-10: Train: Loss-0.0042, Accuracy-0.0926 | Test: Loss-0.0053, Accuracy-0.0497\n",
      "[INFO] Epoch-11-Batch-200: Train: Loss-0.0031, Accuracy-0.0469\n",
      "[INFO] Epoch-11-Batch-400: Train: Loss-0.0080, Accuracy-0.0625\n",
      "[INFO] Epoch-11-Batch-600: Train: Loss-0.0031, Accuracy-0.1250\n",
      "[INFO] Epoch-11-Batch-800: Train: Loss-0.0037, Accuracy-0.1250\n",
      "[INFO] Epoch-11-Batch-1000: Train: Loss-0.0043, Accuracy-0.0312\n",
      "[INFO] Epoch-11-Batch-1200: Train: Loss-0.0065, Accuracy-0.1406\n",
      "[INFO] Epoch-11-Batch-1400: Train: Loss-0.0029, Accuracy-0.0312\n",
      "[INFO] Epoch-11-Batch-1600: Train: Loss-0.0039, Accuracy-0.0781\n",
      "[INFO] Epoch-11-Batch-1800: Train: Loss-0.0026, Accuracy-0.1094\n",
      "[INFO] Epoch-11-Batch-2000: Train: Loss-0.0041, Accuracy-0.0625\n",
      "[INFO] Epoch-11: Train: Loss-0.0042, Accuracy-0.0949 | Test: Loss-0.0043, Accuracy-0.1623\n",
      "[INFO] Epoch-12-Batch-200: Train: Loss-0.0026, Accuracy-0.0938\n",
      "[INFO] Epoch-12-Batch-400: Train: Loss-0.0021, Accuracy-0.1250\n",
      "[INFO] Epoch-12-Batch-600: Train: Loss-0.0028, Accuracy-0.0469\n",
      "[INFO] Epoch-12-Batch-800: Train: Loss-0.0056, Accuracy-0.0781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch-12-Batch-1000: Train: Loss-0.0068, Accuracy-0.0469\n",
      "[INFO] Epoch-12-Batch-1200: Train: Loss-0.0029, Accuracy-0.0625\n",
      "[INFO] Epoch-12-Batch-1400: Train: Loss-0.0045, Accuracy-0.1094\n",
      "[INFO] Epoch-12-Batch-1600: Train: Loss-0.0080, Accuracy-0.1250\n",
      "[INFO] Epoch-12-Batch-1800: Train: Loss-0.0036, Accuracy-0.0938\n",
      "[INFO] Epoch-12-Batch-2000: Train: Loss-0.0068, Accuracy-0.0781\n",
      "[INFO] Epoch-12: Train: Loss-0.0041, Accuracy-0.0982 | Test: Loss-0.0045, Accuracy-0.1230\n",
      "[INFO] Epoch-13-Batch-200: Train: Loss-0.0022, Accuracy-0.1094\n",
      "[INFO] Epoch-13-Batch-400: Train: Loss-0.0024, Accuracy-0.1250\n",
      "[INFO] Epoch-13-Batch-600: Train: Loss-0.0030, Accuracy-0.0781\n",
      "[INFO] Epoch-13-Batch-800: Train: Loss-0.0025, Accuracy-0.0938\n",
      "[INFO] Epoch-13-Batch-1000: Train: Loss-0.0015, Accuracy-0.1094\n",
      "[INFO] Epoch-13-Batch-1200: Train: Loss-0.0035, Accuracy-0.1094\n",
      "[INFO] Epoch-13-Batch-1400: Train: Loss-0.0056, Accuracy-0.1094\n",
      "[INFO] Epoch-13-Batch-1600: Train: Loss-0.0040, Accuracy-0.1094\n",
      "[INFO] Epoch-13-Batch-1800: Train: Loss-0.0026, Accuracy-0.0781\n",
      "[INFO] Epoch-13-Batch-2000: Train: Loss-0.0062, Accuracy-0.0156\n",
      "[INFO] Epoch-13: Train: Loss-0.0041, Accuracy-0.0980 | Test: Loss-0.0042, Accuracy-0.1442\n",
      "[INFO] Epoch-14-Batch-200: Train: Loss-0.0032, Accuracy-0.1719\n",
      "[INFO] Epoch-14-Batch-400: Train: Loss-0.0019, Accuracy-0.0781\n",
      "[INFO] Epoch-14-Batch-600: Train: Loss-0.0064, Accuracy-0.1406\n",
      "[INFO] Epoch-14-Batch-800: Train: Loss-0.0048, Accuracy-0.1094\n",
      "[INFO] Epoch-14-Batch-1000: Train: Loss-0.0042, Accuracy-0.0938\n",
      "[INFO] Epoch-14-Batch-1200: Train: Loss-0.0022, Accuracy-0.1094\n",
      "[INFO] Epoch-14-Batch-1400: Train: Loss-0.0019, Accuracy-0.0938\n",
      "[INFO] Epoch-14-Batch-1600: Train: Loss-0.0044, Accuracy-0.0938\n",
      "[INFO] Epoch-14-Batch-1800: Train: Loss-0.0021, Accuracy-0.1406\n",
      "[INFO] Epoch-14-Batch-2000: Train: Loss-0.0026, Accuracy-0.0938\n",
      "[INFO] Epoch-14: Train: Loss-0.0041, Accuracy-0.0969 | Test: Loss-0.0044, Accuracy-0.0976\n",
      "[INFO] Epoch-15-Batch-200: Train: Loss-0.0057, Accuracy-0.0938\n",
      "[INFO] Epoch-15-Batch-400: Train: Loss-0.0067, Accuracy-0.0625\n",
      "[INFO] Epoch-15-Batch-600: Train: Loss-0.0050, Accuracy-0.1094\n",
      "[INFO] Epoch-15-Batch-800: Train: Loss-0.0040, Accuracy-0.0938\n",
      "[INFO] Epoch-15-Batch-1000: Train: Loss-0.0032, Accuracy-0.0469\n",
      "[INFO] Epoch-15-Batch-1200: Train: Loss-0.0035, Accuracy-0.0625\n",
      "[INFO] Epoch-15-Batch-1400: Train: Loss-0.0050, Accuracy-0.1562\n",
      "[INFO] Epoch-15-Batch-1600: Train: Loss-0.0080, Accuracy-0.0781\n",
      "[INFO] Epoch-15-Batch-1800: Train: Loss-0.0027, Accuracy-0.1094\n",
      "[INFO] Epoch-15-Batch-2000: Train: Loss-0.0049, Accuracy-0.0938\n",
      "[INFO] Epoch-15: Train: Loss-0.0040, Accuracy-0.0992 | Test: Loss-0.0044, Accuracy-0.1113\n",
      "[INFO] Epoch-16-Batch-200: Train: Loss-0.0025, Accuracy-0.1719\n",
      "[INFO] Epoch-16-Batch-400: Train: Loss-0.0014, Accuracy-0.1562\n",
      "[INFO] Epoch-16-Batch-600: Train: Loss-0.0038, Accuracy-0.0625\n",
      "[INFO] Epoch-16-Batch-800: Train: Loss-0.0029, Accuracy-0.1094\n",
      "[INFO] Epoch-16-Batch-1000: Train: Loss-0.0033, Accuracy-0.1250\n",
      "[INFO] Epoch-16-Batch-1200: Train: Loss-0.0039, Accuracy-0.1719\n",
      "[INFO] Epoch-16-Batch-1400: Train: Loss-0.0056, Accuracy-0.1719\n",
      "[INFO] Epoch-16-Batch-1600: Train: Loss-0.0044, Accuracy-0.0312\n",
      "[INFO] Epoch-16-Batch-1800: Train: Loss-0.0039, Accuracy-0.1562\n",
      "[INFO] Epoch-16-Batch-2000: Train: Loss-0.0057, Accuracy-0.1406\n",
      "[INFO] Epoch-16: Train: Loss-0.0040, Accuracy-0.0978 | Test: Loss-0.0044, Accuracy-0.0970\n",
      "[INFO] Epoch-17-Batch-200: Train: Loss-0.0038, Accuracy-0.1875\n",
      "[INFO] Epoch-17-Batch-400: Train: Loss-0.0038, Accuracy-0.0938\n",
      "[INFO] Epoch-17-Batch-600: Train: Loss-0.0044, Accuracy-0.1250\n",
      "[INFO] Epoch-17-Batch-800: Train: Loss-0.0032, Accuracy-0.0469\n",
      "[INFO] Epoch-17-Batch-1000: Train: Loss-0.0052, Accuracy-0.1406\n",
      "[INFO] Epoch-17-Batch-1200: Train: Loss-0.0030, Accuracy-0.0938\n",
      "[INFO] Epoch-17-Batch-1400: Train: Loss-0.0072, Accuracy-0.1562\n",
      "[INFO] Epoch-17-Batch-1600: Train: Loss-0.0071, Accuracy-0.0469\n",
      "[INFO] Epoch-17-Batch-1800: Train: Loss-0.0022, Accuracy-0.1406\n",
      "[INFO] Epoch-17-Batch-2000: Train: Loss-0.0035, Accuracy-0.0781\n",
      "[INFO] Epoch-17: Train: Loss-0.0040, Accuracy-0.0993 | Test: Loss-0.0044, Accuracy-0.1238\n",
      "[INFO] Epoch-18-Batch-200: Train: Loss-0.0032, Accuracy-0.0625\n",
      "[INFO] Epoch-18-Batch-400: Train: Loss-0.0042, Accuracy-0.1094\n",
      "[INFO] Epoch-18-Batch-600: Train: Loss-0.0037, Accuracy-0.1250\n",
      "[INFO] Epoch-18-Batch-800: Train: Loss-0.0022, Accuracy-0.1875\n",
      "[INFO] Epoch-18-Batch-1000: Train: Loss-0.0054, Accuracy-0.0938\n",
      "[INFO] Epoch-18-Batch-1200: Train: Loss-0.0044, Accuracy-0.1562\n",
      "[INFO] Epoch-18-Batch-1400: Train: Loss-0.0034, Accuracy-0.0781\n",
      "[INFO] Epoch-18-Batch-1600: Train: Loss-0.0027, Accuracy-0.1562\n",
      "[INFO] Epoch-18-Batch-1800: Train: Loss-0.0061, Accuracy-0.0781\n",
      "[INFO] Epoch-18-Batch-2000: Train: Loss-0.0039, Accuracy-0.1094\n",
      "[INFO] Epoch-18: Train: Loss-0.0040, Accuracy-0.1005 | Test: Loss-0.0044, Accuracy-0.1088\n",
      "[INFO] Epoch-19-Batch-200: Train: Loss-0.0040, Accuracy-0.0156\n",
      "[INFO] Epoch-19-Batch-400: Train: Loss-0.0042, Accuracy-0.0469\n",
      "[INFO] Epoch-19-Batch-600: Train: Loss-0.0049, Accuracy-0.0938\n",
      "[INFO] Epoch-19-Batch-800: Train: Loss-0.0035, Accuracy-0.0938\n",
      "[INFO] Epoch-19-Batch-1000: Train: Loss-0.0061, Accuracy-0.0625\n",
      "[INFO] Epoch-19-Batch-1200: Train: Loss-0.0049, Accuracy-0.1094\n",
      "[INFO] Epoch-19-Batch-1400: Train: Loss-0.0030, Accuracy-0.1406\n",
      "[INFO] Epoch-19-Batch-1600: Train: Loss-0.0044, Accuracy-0.0938\n",
      "[INFO] Epoch-19-Batch-1800: Train: Loss-0.0059, Accuracy-0.1094\n",
      "[INFO] Epoch-19-Batch-2000: Train: Loss-0.0034, Accuracy-0.0469\n",
      "[INFO] Epoch-19: Train: Loss-0.0040, Accuracy-0.0986 | Test: Loss-0.0047, Accuracy-0.0711\n",
      "[INFO] Epoch-20-Batch-200: Train: Loss-0.0060, Accuracy-0.0781\n",
      "[INFO] Epoch-20-Batch-400: Train: Loss-0.0030, Accuracy-0.0625\n",
      "[INFO] Epoch-20-Batch-600: Train: Loss-0.0032, Accuracy-0.0469\n",
      "[INFO] Epoch-20-Batch-800: Train: Loss-0.0022, Accuracy-0.0781\n",
      "[INFO] Epoch-20-Batch-1000: Train: Loss-0.0061, Accuracy-0.1250\n",
      "[INFO] Epoch-20-Batch-1200: Train: Loss-0.0043, Accuracy-0.1250\n",
      "[INFO] Epoch-20-Batch-1400: Train: Loss-0.0088, Accuracy-0.1094\n",
      "[INFO] Epoch-20-Batch-1600: Train: Loss-0.0039, Accuracy-0.1094\n",
      "[INFO] Epoch-20-Batch-1800: Train: Loss-0.0038, Accuracy-0.0469\n",
      "[INFO] Epoch-20-Batch-2000: Train: Loss-0.0044, Accuracy-0.1094\n",
      "[INFO] Epoch-20: Train: Loss-0.0040, Accuracy-0.1000 | Test: Loss-0.0043, Accuracy-0.1307\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "# 记录训练损失\n",
    "losses = []\n",
    "# 记录训练精度\n",
    "acces = []\n",
    "# 记录测试损失\n",
    "eval_losses = []\n",
    "# 记录测试精度\n",
    "eval_acces = []\n",
    "# 设置迭代次数\n",
    "nums_epoch = 20\n",
    "# 设置准确度小数点\n",
    "dot = 100\n",
    "\n",
    "for epoch in range(nums_epoch):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net = net.train()\n",
    "    for batch, (input, fuel) in enumerate(train_data):    \n",
    "        input = input.to(torch.float32)\n",
    "        fuel = fuel.to(torch.float32)\n",
    "        input = input.reshape(input.size(0), -1)    #标准化\n",
    "        input = Variable(input)    #包装张量，方便反向传播\n",
    "        fuel = Variable(fuel)\n",
    "\n",
    "        # 前向传播\n",
    "        out = net(input)\n",
    "        loss = criterion(out, fuel)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 记录误差\n",
    "        train_loss += loss.item()\n",
    "        # 计算分类的准确率\n",
    "        pred = out\n",
    "        num_correct = 0\n",
    "        for i in range(input.shape[0]):\n",
    "            if (math.floor(pred[i].item()*dot)/dot) == (math.floor(fuel[i].item()*dot)/dot):\n",
    "                num_correct += 1\n",
    "        acc = num_correct / input.shape[0]\n",
    "\n",
    "        if (batch + 1) % 200 == 0:\n",
    "            print('[INFO] Epoch-{}-Batch-{}: Train: Loss-{:.4f}, Accuracy-{:.4f}'.format(epoch + 1,\n",
    "                                                                                 batch+1,\n",
    "                                                                                 loss.item(),\n",
    "                                                                                 acc))\n",
    "            #print (optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        train_acc += acc\n",
    "\n",
    "    losses.append(train_loss / len(train_data))\n",
    "    acces.append(train_acc / len(train_data))\n",
    "\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    \n",
    "    \n",
    "    # 测试集不训练\n",
    "    for batch, (input, fuel) in enumerate(test_data):     \n",
    "        input = input.to(torch.float32)\n",
    "        fuel = fuel.to(torch.float32)\n",
    "        input = input.reshape(input.size(0), -1)    \n",
    "        input = Variable(input)   \n",
    "        fuel = Variable(fuel)\n",
    "\n",
    "        # 前向传播\n",
    "        out = net(input)\n",
    "        loss = criterion(out, fuel)\n",
    "        # 记录误差\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        pred = out\n",
    "        num_correct = 0\n",
    "        for i in range(input.shape[0]):\n",
    "            if (math.floor(pred[i].item()*dot)/dot) == (math.floor(fuel[i].item()*dot)/dot):\n",
    "                num_correct += 1\n",
    "        acc = num_correct / input.shape[0]\n",
    "        \n",
    "        eval_acc += acc\n",
    "    eval_losses.append(eval_loss / len(test_data))\n",
    "    eval_acces.append(eval_acc / len(test_data))\n",
    "\n",
    "    print('[INFO] Epoch-{}: Train: Loss-{:.4f}, Accuracy-{:.4f} | Test: Loss-{:.4f}, Accuracy-{:.4f}'.format(\n",
    "        epoch + 1, train_loss / len(train_data), train_acc / len(train_data), eval_loss / len(test_data),\n",
    "        eval_acc / len(test_data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.0663e-01, -2.3303e-02,  1.0514e-02, -3.7528e-03],\n",
      "        [ 7.4402e-01, -4.2007e-01, -3.8633e-02, -3.1360e-01],\n",
      "        [-1.2182e+00,  4.2501e-01, -4.8920e-02,  1.2880e-01],\n",
      "        ...,\n",
      "        [-6.7759e-01,  4.7041e-01,  1.3476e-02, -9.8330e-02],\n",
      "        [-2.1521e-01, -1.2400e-01, -7.9799e-04,  1.2889e-03],\n",
      "        [-1.0696e+00,  7.5012e-01, -7.2660e-02, -7.6600e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-4.2052e-01,  2.8667e-01, -2.4842e-01, -1.7683e-01, -7.9499e-01,\n",
      "        -8.1494e-01, -4.4554e-01,  1.0929e+00, -9.1288e-01, -4.2104e-01,\n",
      "        -1.8459e-01, -4.3547e-01, -4.2210e-01,  1.3691e-01,  5.0711e-02,\n",
      "        -9.4546e-02, -3.0267e-01, -6.3564e-01,  1.0438e-01,  9.1137e-02,\n",
      "        -1.9526e-01,  2.9134e-02, -7.4423e-01, -7.2616e-01, -2.5562e-01,\n",
      "         3.8672e-01,  1.7511e-01,  4.2487e-01, -5.5771e-01,  2.2432e-01,\n",
      "        -4.1084e-02, -1.4653e-01, -4.8012e-02, -1.0205e+00, -3.9728e-01,\n",
      "        -6.3266e-01,  2.2195e-01, -4.4471e-01,  1.3747e-01, -2.3217e-01,\n",
      "        -1.1164e-01, -7.4539e-01, -3.1016e-01, -5.4608e-01, -4.3539e-01,\n",
      "        -4.9534e-01,  1.0837e-01, -4.1016e-01, -9.4536e-01,  5.0652e-02,\n",
      "         1.6225e-01,  2.2052e-01, -1.7761e-01, -2.0564e-01, -7.4671e-01,\n",
      "         2.2236e-01,  1.5317e-01, -4.3860e-02, -5.7914e-01, -5.0406e-02,\n",
      "         3.3464e-02, -3.6482e-01,  3.2353e-01, -4.6304e-01,  7.0367e-01,\n",
      "         9.6035e-02, -3.9234e-01,  1.4827e-02, -9.4832e-01, -4.8624e-01,\n",
      "        -3.1078e-01, -1.8243e-01,  5.5351e-01, -5.1841e-01,  3.2496e-01,\n",
      "        -7.7236e-01, -9.3708e-01, -2.3129e-01, -4.5584e-01, -9.0357e-01,\n",
      "         3.9098e-01, -2.2593e-01, -9.3760e-02, -1.8962e-01, -6.5259e-03,\n",
      "        -3.9900e-01, -7.8447e-01, -3.0087e-02,  2.4288e-01,  2.0288e-03,\n",
      "        -4.0044e-01, -4.3698e-02, -5.4835e-01, -8.0001e-01, -6.6152e-01,\n",
      "        -1.7925e-01,  2.4422e-01, -2.1510e-02, -6.5371e-01,  4.1461e-01,\n",
      "        -4.0081e-01, -4.0669e-01, -8.7738e-01, -6.0831e-01, -2.2933e-01,\n",
      "         6.1617e-01,  2.5477e-01,  5.4737e-01,  1.5567e-01, -6.2154e-01,\n",
      "         4.5010e-01,  6.6036e-01, -5.0469e-01, -2.0798e-01, -6.0059e-01,\n",
      "        -3.3755e-01,  2.8896e-01, -1.9777e-01, -1.3167e-01, -8.2231e-01,\n",
      "        -3.0274e-02, -2.2652e-01, -3.3380e-01,  5.1701e-01,  1.5038e-01,\n",
      "        -2.5854e-01, -4.8923e-01, -3.9095e-01, -9.4839e-01, -5.5000e-01,\n",
      "         4.4788e-01, -5.7350e-02,  7.6990e-02,  2.7940e-01, -1.6497e-01,\n",
      "         1.1211e-01, -5.4763e-01, -5.6689e-02, -2.2550e-01,  1.5067e-02,\n",
      "        -1.5894e-01, -8.7140e-01, -5.6936e-01, -6.4301e-03, -1.0006e-01,\n",
      "        -3.6785e-01, -7.4038e-02, -3.8246e-01, -1.3777e-01, -3.6181e-01,\n",
      "         8.6126e-03, -2.0439e-01, -3.5684e-01,  2.0561e-01, -5.5261e-01,\n",
      "        -3.9254e-01, -3.1962e-01,  3.2570e-01, -1.5387e-01, -1.4268e-01,\n",
      "         4.4108e-01,  7.1318e-01, -4.2850e-02,  4.8101e-01,  4.8214e-02,\n",
      "        -2.8825e-01,  5.5286e-01,  3.5036e-01, -3.7737e-01,  1.1119e-03,\n",
      "         5.3917e-02, -3.1502e-01,  4.9043e-02, -5.2159e-02,  4.4481e-01,\n",
      "         1.6974e-01, -8.7593e-02, -2.6419e-01, -2.4334e-01, -1.2918e-01,\n",
      "        -1.4457e-01, -5.3910e-01,  5.2122e-01, -5.3171e-01, -1.2345e-01,\n",
      "        -9.9384e-01, -9.4872e-01,  1.1749e-01, -8.3223e-01, -2.7709e-01,\n",
      "         7.7606e-01, -4.9488e-01, -4.2415e-01, -1.0305e-01, -8.1393e-01,\n",
      "        -3.1921e-01, -5.0290e-01, -3.6423e-01, -5.7941e-01, -2.1783e-01,\n",
      "        -7.0379e-02, -7.0146e-01, -3.3277e-01, -2.5842e-01, -1.3897e-01,\n",
      "        -6.9162e-01, -2.0829e-01, -8.3532e-02, -8.5099e-02, -8.1925e-01,\n",
      "         8.2994e-02, -2.6797e-01,  1.4515e-01, -1.2977e-01, -5.8594e-01,\n",
      "        -6.6798e-01,  6.1917e-01, -6.3563e-02, -2.6890e-01,  2.6444e-01,\n",
      "         1.3798e-01,  4.1572e-01, -2.4559e-01, -6.8515e-01,  1.7667e-01,\n",
      "         1.3235e-01, -4.8370e-02, -3.5217e-01, -1.4151e-01, -2.5415e-01,\n",
      "        -2.4899e-01, -5.8854e-01, -8.0524e-01, -2.7252e-01, -1.7434e-01,\n",
      "        -7.6962e-01,  5.4728e-01, -9.3597e-02,  2.0342e-01,  6.9400e-01,\n",
      "        -4.3203e-01, -4.1278e-01,  3.3520e-01, -6.3954e-01,  1.5005e-01,\n",
      "        -2.3053e-01, -2.2918e-01,  8.6960e-02, -7.9455e-01,  1.8055e-02,\n",
      "        -3.1100e-01,  4.0362e-01,  1.3320e-01, -2.7897e-01,  6.4006e-02,\n",
      "        -8.1445e-01, -1.6780e-01,  2.9699e-01, -2.4820e-01, -6.3710e-01,\n",
      "        -6.0800e-01,  6.5562e-01, -8.9279e-02, -3.2669e-01, -2.4518e-01,\n",
      "        -4.6345e-01,  2.0655e-01, -7.1930e-01, -1.7218e-02,  2.0003e-01,\n",
      "        -3.0468e-01,  5.4494e-01,  4.5027e-01, -4.0660e-01, -3.8703e-01,\n",
      "         5.2603e-01,  2.3230e-01,  4.1861e-01, -6.3754e-01, -5.3223e-01,\n",
      "         3.7781e-01, -4.4289e-01, -6.1856e-01, -3.8031e-01,  9.4990e-02,\n",
      "        -8.9225e-01,  4.4595e-02, -7.2091e-01,  1.6847e-01, -5.1449e-01,\n",
      "         5.3247e-01, -7.7602e-01, -6.5898e-01,  2.2343e-01, -4.7972e-02,\n",
      "        -6.6361e-01, -2.2785e-01,  9.9956e-02, -1.2466e-02,  2.1836e-01,\n",
      "         4.6874e-01, -2.7349e-01,  6.1377e-01,  1.6094e-01,  6.0013e-01,\n",
      "         1.9843e-01, -1.0256e-01, -4.2868e-01, -1.3085e-01,  4.8705e-01,\n",
      "        -2.7929e-01,  1.8028e-01,  3.9351e-01, -5.0910e-01, -8.5451e-01,\n",
      "         1.8149e-01, -2.4638e-01,  2.1318e-01, -5.9200e-01, -6.7739e-01,\n",
      "        -2.3866e-01, -6.0767e-01,  1.6810e-01, -4.3211e-01,  2.8798e-01,\n",
      "        -1.1561e-01, -7.9635e-01, -2.6670e-01,  8.0935e-02, -1.4935e-02,\n",
      "         5.6762e-01, -9.5502e-01, -1.7783e-01, -7.1731e-01, -5.9647e-01,\n",
      "         2.6304e-01,  9.3430e-02,  4.5295e-01, -2.8940e-01, -1.1168e+00,\n",
      "        -2.9680e-01, -5.7823e-01, -8.3162e-01, -3.2067e-01,  1.8446e-01,\n",
      "        -2.6896e-01, -2.9117e-01, -7.6480e-01, -2.2001e-01, -4.3967e-01,\n",
      "         2.5873e-01, -6.7441e-02, -3.4634e-01,  1.6093e-01, -5.4359e-01,\n",
      "        -1.1620e-01,  1.4373e-01, -1.8644e-01, -7.8152e-02, -4.1288e-01,\n",
      "        -4.2915e-01,  5.0945e-02, -7.8981e-01, -9.4441e-01, -2.6482e-01,\n",
      "        -6.3434e-02, -1.3422e-01, -5.3121e-02,  3.1619e-01, -6.3792e-01,\n",
      "         1.3287e-01,  5.5710e-01, -6.8618e-01, -5.1171e-01, -7.5242e-02,\n",
      "        -9.5843e-01,  2.3173e-01,  1.8201e-01, -5.6144e-01, -1.7351e-01,\n",
      "        -4.0959e-01, -1.3195e-01,  5.7392e-03,  2.4268e-02, -1.2038e-01,\n",
      "         1.2735e-01, -5.4689e-01,  1.5529e-01,  2.2892e-01, -8.9621e-01,\n",
      "        -1.6424e-01,  3.3526e-01,  1.2481e-02,  5.4169e-01, -8.9748e-01,\n",
      "         1.9736e-01, -6.5627e-03, -2.0975e-01,  1.4531e-01, -1.0379e-01,\n",
      "        -7.8157e-01,  1.5397e-01, -9.5807e-01, -3.2004e-01,  1.8788e-02,\n",
      "        -5.5919e-01, -8.2063e-02,  3.4141e-01,  1.8514e-01, -8.5647e-01,\n",
      "         1.4644e-01, -1.6775e-02, -3.9186e-01,  4.5314e-01, -3.7987e-01,\n",
      "         6.6709e-01, -1.5828e-01,  2.2109e-01, -1.8054e-01, -6.0762e-01,\n",
      "         1.2942e-01, -6.0001e-01,  4.3987e-01, -7.5232e-01, -9.7205e-01,\n",
      "         7.6317e-03,  2.8390e-01, -2.8139e-01, -6.5219e-01,  1.9382e-01,\n",
      "         8.9055e-02,  7.4087e-01, -2.8451e-01, -5.5381e-01, -2.0107e-01,\n",
      "         4.3060e-02,  5.9879e-01,  8.1553e-01, -1.3180e-01, -3.4643e-01,\n",
      "        -3.0050e-01, -6.0966e-01,  4.6305e-01, -3.7696e-01, -4.9547e-02,\n",
      "        -6.9700e-01,  1.1992e-01, -1.5259e-01, -3.6817e-01, -3.4525e-03,\n",
      "         5.9480e-01,  1.2464e-01, -3.3941e-01,  1.1471e-01, -2.9274e-01,\n",
      "        -7.4937e-01,  4.8386e-01, -9.5441e-01, -6.6731e-01,  1.3487e-02,\n",
      "        -7.3703e-01,  6.5397e-01, -4.7875e-01,  3.6561e-01,  8.6832e-02,\n",
      "         5.0288e-03,  1.4946e-01, -1.2081e-01,  2.5420e-02, -7.5504e-02,\n",
      "        -1.6292e-01, -4.2547e-01, -1.4137e-01, -7.1328e-01, -9.1321e-02,\n",
      "         4.6321e-01, -8.2355e-01,  1.8817e-01, -1.0724e-01, -2.9344e-01,\n",
      "        -2.6145e-01,  5.2902e-01, -2.1836e-01, -6.6108e-01, -5.4879e-01,\n",
      "        -1.8615e-01, -1.5357e-01, -1.8022e-01, -3.7560e-01, -5.7390e-01,\n",
      "        -7.6249e-01, -3.7774e-01, -2.8902e-02, -1.1032e-01, -2.6539e-01,\n",
      "        -1.3746e-01,  3.2415e-02,  7.4392e-02,  2.0542e-01,  8.7593e-01,\n",
      "        -2.2961e-01, -8.7191e-01, -5.2714e-02, -6.5433e-02, -2.5936e-01,\n",
      "        -5.5020e-01, -1.0241e-01, -6.4967e-02, -2.7012e-01, -3.6547e-02,\n",
      "        -2.2612e-01,  7.3075e-01], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0381, -0.0323, -0.0441,  ...,  0.0310, -0.0186,  0.0247],\n",
      "        [ 0.0220, -0.0247, -0.0508,  ...,  0.0336,  0.0292, -0.0081],\n",
      "        [-0.0191, -0.0236, -0.0966,  ...,  0.0028, -0.0333, -0.0858],\n",
      "        ...,\n",
      "        [ 0.0216,  0.0060, -0.0414,  ...,  0.0341, -0.0255, -0.0179],\n",
      "        [ 0.0145,  0.0103, -0.0390,  ...,  0.0029,  0.0081,  0.0079],\n",
      "        [ 0.0190, -0.0092, -0.0388,  ...,  0.0146, -0.0460,  0.0087]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.9844e-02, -3.4654e-02, -4.0388e-02,  2.5787e-02, -2.3890e-02,\n",
      "         2.2698e-01,  7.3693e-03, -8.3594e-02,  5.9817e-02, -4.1541e-02,\n",
      "        -4.0454e-02,  1.6357e-01,  3.7813e-05, -2.5317e-04,  8.2616e-02,\n",
      "         2.2304e-01, -3.3775e-02, -2.0684e-02,  1.4599e-01, -6.5000e-02,\n",
      "        -3.5164e-02, -3.0008e-02,  3.0252e-02, -4.4998e-02, -1.0527e-03,\n",
      "        -4.6762e-02, -2.0901e-02,  1.2960e-01, -4.1467e-02,  3.5651e-02,\n",
      "        -1.6654e-02, -2.5137e-02,  1.7959e-02,  2.8978e-02, -1.2175e-02,\n",
      "        -1.3946e-04,  3.5006e-02,  1.3101e-01, -1.4284e-02,  1.7920e-02,\n",
      "        -1.1425e-02,  7.2791e-02, -2.0477e-02,  6.6747e-02,  5.7006e-02,\n",
      "        -3.1426e-02, -4.6487e-02,  1.8084e-02,  6.7615e-03,  1.2709e-02,\n",
      "        -3.1118e-02, -3.0465e-02, -3.1742e-02,  8.5714e-02, -4.7321e-02,\n",
      "         8.9400e-02, -5.5018e-02, -5.6604e-02, -6.9172e-03,  1.7367e-02,\n",
      "         4.3125e-03, -3.5952e-02, -1.7282e-03, -4.6413e-02, -2.6812e-02,\n",
      "        -6.9662e-02,  1.9732e-01, -6.5216e-02,  3.1374e-01,  9.2847e-02,\n",
      "         1.0230e-02, -6.3817e-02,  1.5870e-01, -5.5599e-03,  6.1181e-02,\n",
      "         7.8662e-03,  2.1281e-01,  3.0932e-03, -5.4551e-02,  1.4149e-02,\n",
      "        -4.5210e-02, -5.5018e-02, -8.1910e-02,  1.6674e-02, -6.1260e-02,\n",
      "         4.9561e-02, -2.7847e-02,  4.7313e-01, -1.7891e-02,  2.6523e-01,\n",
      "         1.3899e-02, -4.2152e-02, -4.5881e-02,  2.4241e-02, -8.7376e-03,\n",
      "         5.1553e-04, -3.4228e-02,  9.1886e-03, -4.5069e-02, -1.7482e-02,\n",
      "        -4.5713e-02,  2.2505e-01, -8.3613e-02, -2.9873e-02, -1.0182e-02,\n",
      "        -7.8153e-02,  1.0902e-01, -4.1714e-02, -2.6306e-02, -3.4230e-02,\n",
      "        -4.5349e-02,  8.2960e-03,  3.9343e-02, -5.7436e-03, -4.8530e-02,\n",
      "         3.6880e-01,  1.3479e-02, -1.8020e-02,  4.5947e-04, -5.0503e-04,\n",
      "        -5.4110e-02, -1.4980e-02, -2.6975e-02,  4.4119e-02, -1.0625e-02,\n",
      "        -4.7509e-02, -1.1897e-02, -1.5178e-02, -1.2924e-02, -4.0184e-02,\n",
      "         3.9004e-02,  2.8288e-02,  1.1810e-01, -2.6220e-02, -6.9567e-02,\n",
      "         1.7468e-02, -1.3553e-01,  1.1075e-02,  2.3443e-04, -5.7043e-02,\n",
      "        -4.5047e-02,  3.0554e-02, -2.4402e-02,  1.5573e-01, -4.6767e-02,\n",
      "         3.1275e-02,  8.5537e-02,  6.2054e-03,  3.5996e-02, -3.2659e-02,\n",
      "        -2.6929e-02, -1.1288e-03, -9.0159e-02,  4.8533e-02, -1.5894e-02,\n",
      "         1.3970e-01,  9.5466e-04,  1.6403e-01, -4.5531e-02, -2.2035e-02,\n",
      "         1.7260e-02,  9.2790e-03,  2.6577e-02, -7.7958e-02,  4.5284e-02,\n",
      "        -4.8513e-03, -3.0521e-02, -8.1591e-02,  3.0198e-02, -1.2896e-02,\n",
      "         2.0834e-01, -1.0856e-02,  1.8212e-02, -2.3802e-02, -3.0430e-02,\n",
      "        -2.0176e-02,  1.3681e-01,  2.5549e-01, -5.0184e-03,  1.1054e-02,\n",
      "        -4.2472e-02, -5.2892e-02, -1.1750e-01, -4.7599e-02, -5.9403e-02,\n",
      "        -7.0250e-02,  7.6444e-03, -1.8886e-02, -1.6170e-02, -3.0227e-02,\n",
      "        -6.5747e-02, -2.2649e-02, -6.3182e-02,  2.2318e-02, -7.5308e-03,\n",
      "        -2.3180e-03,  9.1401e-03, -5.7973e-03,  1.4846e-01,  1.0551e-01,\n",
      "        -2.5990e-02, -5.0407e-02,  4.2101e-03,  8.2555e-02, -1.6757e-02,\n",
      "        -4.6357e-02, -1.4653e-02,  1.3772e-02,  2.6141e-04,  6.5154e-02,\n",
      "        -6.0059e-02,  8.6909e-02,  2.3043e-03,  3.1775e-02, -1.3629e-03,\n",
      "         2.3878e-02, -9.2876e-02, -3.5987e-03,  2.2107e-02, -1.8755e-02,\n",
      "         2.2186e-02,  1.0770e-02, -5.7189e-02, -3.6007e-02, -1.8441e-02,\n",
      "         3.2859e-02, -4.9342e-03, -1.8338e-03, -2.3876e-03, -1.2588e-01,\n",
      "         1.5688e-01,  2.4861e-01, -8.2754e-02, -7.6017e-03,  1.1626e-01,\n",
      "        -2.1571e-02,  1.8770e-02,  9.9929e-03, -1.4484e-02,  1.5697e-01,\n",
      "         1.5241e-02,  2.2174e-02,  3.8552e-02,  5.5795e-02, -4.9050e-02,\n",
      "         1.3043e-02, -2.3413e-02, -1.3968e-02,  1.7726e-01,  5.4562e-03,\n",
      "        -1.4956e-02, -2.2623e-02,  1.0582e-03, -5.5907e-02, -2.0314e-02,\n",
      "        -3.0004e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0023, -0.0518,  0.0240,  ...,  0.0438,  0.0618, -0.0632],\n",
      "        [ 0.0546,  0.0396,  0.0294,  ..., -0.0180,  0.0021,  0.0276],\n",
      "        [-0.0607, -0.0027, -0.0200,  ..., -0.0420,  0.0435,  0.0193],\n",
      "        ...,\n",
      "        [ 0.0234,  0.0194,  0.0024,  ..., -0.0144, -0.0486, -0.0190],\n",
      "        [ 0.0008,  0.0234,  0.0039,  ..., -0.0446,  0.0083,  0.0111],\n",
      "        [ 0.0361,  0.0480,  0.0573,  ...,  0.0590, -0.0024,  0.0171]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1306, -0.1189, -0.1201, -0.1264,  0.0840, -0.0273, -0.0579, -0.1769,\n",
      "        -0.0262, -0.1744,  0.1429, -0.1843, -0.0217, -0.1001, -0.1218,  0.1766,\n",
      "        -0.1061,  0.2872,  0.0504, -0.0913, -0.0075, -0.0926, -0.1378, -0.0736,\n",
      "        -0.0683, -0.1329, -0.0699, -0.1274, -0.2761, -0.0410, -0.3044, -0.0225,\n",
      "        -0.1269, -0.0748, -0.1347, -0.0569, -0.0873, -0.1286, -0.2021, -0.1898,\n",
      "        -0.1805, -0.2238, -0.1313, -0.1636, -0.0643, -0.1465, -0.1127, -0.1222,\n",
      "        -0.0837, -0.2207,  0.1345, -0.2201, -0.1030, -0.0648, -0.0403, -0.1855,\n",
      "         0.1419, -0.1032, -0.0344,  0.1724, -0.0938, -0.0793,  0.5546, -0.0312,\n",
      "        -0.1090, -0.1963, -0.1212,  0.0236,  0.0814, -0.1566, -0.1492, -0.1989,\n",
      "         0.1709, -0.0008, -0.1102,  0.4240, -0.1090,  0.1464,  0.1386, -0.3488,\n",
      "         0.0709, -0.1374, -0.2999, -0.0950, -0.1225, -0.1050, -0.0255, -0.0944,\n",
      "        -0.4788, -0.0698, -0.2006, -0.0411, -0.1274, -0.3046, -0.0589, -0.0809,\n",
      "        -0.1079, -0.1303, -0.1474,  0.2799, -0.0093, -0.0672,  0.1088,  0.0408,\n",
      "        -0.1413, -0.2323, -0.1493, -0.1841,  0.0614,  0.6489,  0.0932, -0.0542,\n",
      "        -0.1996,  0.1066,  0.1191, -0.1717, -0.2579, -0.0930,  0.0510, -0.0567,\n",
      "        -0.1308, -0.0205, -0.1481, -0.0972, -0.1563, -0.0956, -0.1608,  0.1548],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.7630e-02, -7.8029e-03, -6.0438e-02, -2.8014e-02,  6.9723e-02,\n",
      "         -1.3206e-02, -5.2151e-02,  1.3576e-02,  4.0986e-04, -6.3438e-03,\n",
      "          6.1985e-02,  4.0168e-04,  2.2230e-02,  1.4773e-02,  2.5529e-02,\n",
      "          2.0771e-02,  1.1440e-02,  2.0087e-02,  2.5140e-02,  2.2372e-02,\n",
      "         -1.0120e-02,  2.5756e-02, -8.7567e-04, -5.7492e-02,  1.6497e-04,\n",
      "         -1.7375e-02, -2.1476e-02, -3.8261e-02, -6.6456e-02, -1.9462e-02,\n",
      "          3.5398e-02,  3.2190e-02,  2.0187e-02, -5.9988e-02, -1.7397e-02,\n",
      "         -3.8883e-02, -2.3045e-02, -1.2557e-02, -4.5034e-02, -3.3896e-02,\n",
      "         -2.9003e-02, -1.1540e-02, -1.1924e-02, -2.7879e-02, -3.4357e-02,\n",
      "          1.1201e-03, -2.6758e-02,  6.0196e-02, -4.0947e-02, -6.4795e-03,\n",
      "          4.0580e-02, -4.6614e-02, -2.7058e-02, -6.6237e-02, -1.9290e-02,\n",
      "          8.2867e-03,  7.9425e-03,  2.1972e-03,  7.0357e-02,  7.1008e-02,\n",
      "         -5.9773e-03,  3.0220e-02,  3.9911e-02,  3.0689e-02,  3.8723e-02,\n",
      "         -5.1505e-03, -9.6093e-03, -2.4978e-02,  2.9359e-02, -2.7226e-02,\n",
      "         -8.2459e-03, -6.5851e-02, -1.9699e-02, -9.4370e-02, -1.1913e-04,\n",
      "          5.8504e-02, -2.0245e-02,  1.5732e-03,  9.0748e-02,  5.2852e-03,\n",
      "          6.4248e-02,  1.3855e-02, -2.2465e-02, -3.9185e-02,  1.7825e-02,\n",
      "         -2.8820e-02, -1.4464e-02, -4.5704e-02,  5.9500e-03,  1.5875e-02,\n",
      "         -1.7166e-02, -4.6726e-03, -1.8560e-02, -1.2477e-02,  6.4839e-02,\n",
      "         -4.4482e-02, -1.9276e-02, -1.9316e-02, -1.5876e-02,  4.8889e-02,\n",
      "         -1.7607e-02,  2.0421e-02, -3.3590e-02, -1.2785e-03,  4.2210e-02,\n",
      "         -2.5116e-02, -1.8683e-02, -2.8589e-02,  7.1055e-03,  7.4889e-02,\n",
      "         -6.4759e-03,  3.4778e-02, -8.5725e-04,  3.9339e-02,  1.2691e-01,\n",
      "         -2.6982e-02,  3.1916e-02, -1.3278e-02, -1.6153e-02,  3.0333e-02,\n",
      "          4.4886e-03, -2.7965e-02, -3.5949e-03, -5.2547e-02,  1.7538e-02,\n",
      "         -2.5913e-02, -3.0564e-03, -4.8131e-03],\n",
      "        [-7.1356e-02,  1.7757e-02, -3.2233e-02, -4.7508e-02, -2.4572e-02,\n",
      "          6.0246e-02, -5.0167e-02,  9.1744e-02,  1.0368e-01, -8.4648e-05,\n",
      "         -2.8566e-02,  2.1750e-02, -2.0635e-02,  4.0022e-02,  2.3699e-02,\n",
      "          1.1004e-01,  1.5587e-03,  4.8115e-02, -5.9944e-02, -1.1050e-01,\n",
      "         -3.3022e-02,  3.7094e-02, -3.7559e-02, -7.2237e-02,  2.7697e-02,\n",
      "          1.5349e-01, -8.2836e-05,  6.1731e-02, -2.2802e-02,  1.6877e-01,\n",
      "          1.6769e-02,  2.5402e-02,  1.0706e-01,  4.7158e-02,  6.7541e-02,\n",
      "          2.4626e-04, -4.7819e-03, -1.8373e-02, -4.3159e-03, -1.4695e-02,\n",
      "         -9.4795e-02,  1.3360e-01,  5.1582e-02,  7.7972e-02, -9.6454e-02,\n",
      "         -3.1564e-02, -6.1709e-02, -1.5780e-02,  1.4074e-02, -6.2162e-02,\n",
      "          7.8996e-02, -3.7664e-02, -6.5503e-02,  2.3097e-02,  3.3575e-02,\n",
      "          1.2014e-01,  1.1388e-02,  1.5297e-01,  2.3434e-02,  1.9237e-02,\n",
      "          2.3598e-02, -3.4848e-02, -1.4708e-02,  5.8861e-02, -1.6690e-02,\n",
      "          6.4850e-02,  6.7834e-02,  5.9997e-03, -1.0147e-01,  8.8321e-03,\n",
      "         -2.2074e-03,  8.6482e-02,  1.3456e-01, -2.5572e-02,  1.1045e-01,\n",
      "          2.2069e-02,  5.1508e-02, -1.0264e-02,  1.9439e-02, -2.0804e-02,\n",
      "         -3.2391e-03,  9.7014e-02, -5.5424e-03,  4.9766e-02,  1.2166e-01,\n",
      "         -2.4332e-02,  2.2406e-02,  5.6794e-02, -5.8004e-02,  2.6888e-02,\n",
      "          5.4054e-03, -4.0047e-02, -4.2023e-02,  3.9706e-02,  1.5037e-02,\n",
      "         -6.3054e-02,  4.5866e-02, -4.8434e-02, -7.6866e-02,  7.4692e-02,\n",
      "          2.1773e-03, -2.1316e-02,  5.9818e-02, -1.5920e-02,  7.2290e-02,\n",
      "          3.0866e-02, -4.7135e-02, -5.7321e-02,  8.7060e-02, -2.0722e-02,\n",
      "         -6.4912e-02,  2.0100e-02, -3.2224e-03, -2.3708e-02, -7.4960e-02,\n",
      "         -1.6710e-02,  8.0580e-03, -5.8912e-02, -1.4432e-02, -5.4605e-02,\n",
      "          6.5658e-02,  1.1246e-01,  3.5005e-02, -3.1877e-02, -8.0438e-02,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         -6.8114e-02,  3.1693e-02,  2.0072e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.6039, -0.5222], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3300, -0.0390]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0163], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for para in net.parameters():\n",
    "    print (para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.suptitle('Test', fontsize=12)\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.plot(eval_losses, color='r')\n",
    "ax1.plot(losses, color='b')\n",
    "ax1.set_title('Loss', fontsize=10, color='black')\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.plot(eval_acces, color='r')\n",
    "ax2.plot(acces, color='b')\n",
    "ax2.set_title('Acc', fontsize=10, color='black')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
